{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc # garbage collection module to release memory usage in time\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\frank\\OneDrive - Escuela Superior de Economia y Negocios\\2. SS 2024\\ML Seminar\\data\\Rawdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "## Firm characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load firm characteristics data\n",
    "data_df = pd.read_csv('data_ch_sample.csv')\n",
    "#data_df = pd.read_csv('data_ch_sample_94p.csv')\n",
    "#data_df = pd.read_csv('data_ch_sample_32.csv')\n",
    "#data_df = pd.read_csv('data_ch_sample_5716.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>DATE</th>\n",
       "      <th>mvel1</th>\n",
       "      <th>RET</th>\n",
       "      <th>prc</th>\n",
       "      <th>SHROUT</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>chmom</th>\n",
       "      <th>dolvol</th>\n",
       "      <th>...</th>\n",
       "      <th>sp</th>\n",
       "      <th>nincr</th>\n",
       "      <th>baspread</th>\n",
       "      <th>ill</th>\n",
       "      <th>maxret</th>\n",
       "      <th>retvol</th>\n",
       "      <th>std_turn</th>\n",
       "      <th>zerotrade</th>\n",
       "      <th>sic2</th>\n",
       "      <th>bm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>24355.500</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>9.8750</td>\n",
       "      <td>2498</td>\n",
       "      <td>0.037079</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.281788</td>\n",
       "      <td>8.395576</td>\n",
       "      <td>...</td>\n",
       "      <td>3.646263</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.020711</td>\n",
       "      <td>1.098587e-06</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.017710</td>\n",
       "      <td>0.426715</td>\n",
       "      <td>4.200000e+00</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.868139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>78332.625</td>\n",
       "      <td>0.088435</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>8526</td>\n",
       "      <td>0.206346</td>\n",
       "      <td>0.042579</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>8.067022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428502</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.033991</td>\n",
       "      <td>6.509871e-06</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.054790</td>\n",
       "      <td>0.759666</td>\n",
       "      <td>4.200000e+00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.680296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10012</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>39836.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>20897</td>\n",
       "      <td>2.470629</td>\n",
       "      <td>6.104008</td>\n",
       "      <td>-1.170178</td>\n",
       "      <td>11.360419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172669</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.138777</td>\n",
       "      <td>9.482216e-08</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.075671</td>\n",
       "      <td>7.007556</td>\n",
       "      <td>8.756593e-09</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.061049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10016</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>379569.500</td>\n",
       "      <td>0.030726</td>\n",
       "      <td>23.0625</td>\n",
       "      <td>16964</td>\n",
       "      <td>0.449866</td>\n",
       "      <td>0.202379</td>\n",
       "      <td>0.391222</td>\n",
       "      <td>12.024414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602373</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.054578</td>\n",
       "      <td>5.643552e-08</td>\n",
       "      <td>0.070769</td>\n",
       "      <td>0.040708</td>\n",
       "      <td>8.102766</td>\n",
       "      <td>1.833562e-08</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.287808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10019</td>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>28945.000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>3.7500</td>\n",
       "      <td>8270</td>\n",
       "      <td>2.249729</td>\n",
       "      <td>5.061279</td>\n",
       "      <td>0.203106</td>\n",
       "      <td>9.294773</td>\n",
       "      <td>...</td>\n",
       "      <td>2.811052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131620</td>\n",
       "      <td>3.206363e-07</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.120324</td>\n",
       "      <td>16.163956</td>\n",
       "      <td>7.497863e-09</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.552262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   permno       DATE       mvel1       RET      prc  SHROUT      beta  \\\n",
       "0   10001 2001-01-31   24355.500  0.012821   9.8750    2498  0.037079   \n",
       "1   10002 2001-01-31   78332.625  0.088435  10.0000    8526  0.206346   \n",
       "2   10012 2001-01-31   39836.000  0.500000   3.0000   20897  2.470629   \n",
       "3   10016 2001-01-31  379569.500  0.030726  23.0625   16964  0.449866   \n",
       "4   10019 2001-01-31   28945.000  0.071429   3.7500    8270  2.249729   \n",
       "\n",
       "     betasq     chmom     dolvol  ...        sp  nincr  baspread  \\\n",
       "0  0.001375  0.281788   8.395576  ...  3.646263    2.0  0.020711   \n",
       "1  0.042579  0.050021   8.067022  ...  0.428502    5.0  0.033991   \n",
       "2  6.104008 -1.170178  11.360419  ...  0.172669    7.0  0.138777   \n",
       "3  0.202379  0.391222  12.024414  ...  0.602373    6.0  0.054578   \n",
       "4  5.061279  0.203106   9.294773  ...  2.811052    0.0  0.131620   \n",
       "\n",
       "            ill    maxret    retvol   std_turn     zerotrade  sic2        bm  \n",
       "0  1.098587e-06  0.027778  0.017710   0.426715  4.200000e+00  49.0  0.868139  \n",
       "1  6.509871e-06  0.134328  0.054790   0.759666  4.200000e+00  60.0  0.680296  \n",
       "2  9.482216e-08  0.129412  0.075671   7.007556  8.756593e-09  36.0  0.061049  \n",
       "3  5.643552e-08  0.070769  0.040708   8.102766  1.833562e-08  38.0  0.287808  \n",
       "4  3.206363e-07  0.435897  0.120324  16.163956  7.497863e-09  38.0  0.552262  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ch = data_df.copy()\n",
    "\n",
    "data_ch['DATE'] = pd.to_datetime(data_ch['DATE'],format='%Y%m%d')+pd.offsets.MonthEnd(0)\n",
    "characteristics = list(set(data_ch.columns).difference({'permno','DATE','SHROUT','mve0','sic2','RET','prc'}))\n",
    "\n",
    "data_ch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ch = data_ch.drop(columns=['prc','SHROUT','mve0','sic2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7.2 s\n",
      "Wall time: 8.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fill na with cross-sectional median\n",
    "for ch in characteristics:\n",
    "     data_ch[ch] = data_ch.groupby('DATE')[ch].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ch in characteristics:\n",
    "     data_ch[ch] = data_ch[ch].fillna(0)\n",
    "    \n",
    "data_ch.columns[data_ch.isnull().sum()!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macroeconomic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdt, nddt = 20010101, 20191231\n",
    "data_ma = pd.read_csv('PredictorData2023.csv')\n",
    "data_ma = data_ma[(data_ma['yyyymm']>=stdt//100)&(data_ma['yyyymm']<=nddt//100)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yyyymm</th>\n",
       "      <th>dp_sp</th>\n",
       "      <th>ep_sp</th>\n",
       "      <th>bm_sp</th>\n",
       "      <th>ntis</th>\n",
       "      <th>tbl</th>\n",
       "      <th>tms</th>\n",
       "      <th>dfy</th>\n",
       "      <th>svar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.035490</td>\n",
       "      <td>0.150450</td>\n",
       "      <td>-0.003193</td>\n",
       "      <td>0.0515</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.004941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-02-28</td>\n",
       "      <td>0.012962</td>\n",
       "      <td>0.037873</td>\n",
       "      <td>0.156070</td>\n",
       "      <td>-0.006856</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.002528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-03-31</td>\n",
       "      <td>0.013766</td>\n",
       "      <td>0.039161</td>\n",
       "      <td>0.133114</td>\n",
       "      <td>-0.005213</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.007140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-04-30</td>\n",
       "      <td>0.012707</td>\n",
       "      <td>0.034060</td>\n",
       "      <td>0.122497</td>\n",
       "      <td>-0.002543</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.007426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-05-31</td>\n",
       "      <td>0.012567</td>\n",
       "      <td>0.031592</td>\n",
       "      <td>0.120510</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.002536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      yyyymm     dp_sp     ep_sp     bm_sp      ntis     tbl     tms     dfy  \\\n",
       "0 2001-01-31  0.011839  0.035490  0.150450 -0.003193  0.0515  0.0047  0.0078   \n",
       "1 2001-02-28  0.012962  0.037873  0.156070 -0.006856  0.0488  0.0061  0.0077   \n",
       "2 2001-03-31  0.013766  0.039161  0.133114 -0.005213  0.0442  0.0117  0.0086   \n",
       "3 2001-04-30  0.012707  0.034060  0.122497 -0.002543  0.0387  0.0206  0.0087   \n",
       "4 2001-05-31  0.012567  0.031592  0.120510 -0.000248  0.0362  0.0232  0.0078   \n",
       "\n",
       "       svar  \n",
       "0  0.004941  \n",
       "1  0.002528  \n",
       "2  0.007140  \n",
       "3  0.007426  \n",
       "4  0.002536  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct predictor\n",
    "ma_predictors = ['dp_sp','ep_sp','bm_sp','ntis','tbl','tms','dfy','svar']\n",
    "data_ma['Index'] = data_ma['Index'].str.replace(',','').astype('float64')\n",
    "data_ma['dp_sp'] = data_ma['D12']/data_ma['Index']\n",
    "data_ma['ep_sp'] = data_ma['E12']/data_ma['Index']\n",
    "data_ma.rename({'b/m':'bm_sp'},axis=1,inplace=True)\n",
    "data_ma['tms'] = data_ma['lty']-data_ma['tbl']\n",
    "data_ma['dfy'] = data_ma['BAA']-data_ma['AAA']\n",
    "data_ma = data_ma[['yyyymm']+ma_predictors]\n",
    "data_ma['yyyymm'] = pd.to_datetime(data_ma['yyyymm'],format='%Y%m')+pd.offsets.MonthEnd(0)\n",
    "data_ma.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(data_ch, data_ma, left_on='DATE', right_on='yyyymm', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in characteristics:\n",
    "    for macro_var in ma_predictors:\n",
    "        interaction_term_name = f'{feature}_x_{macro_var}'\n",
    "        merged_data[interaction_term_name] = merged_data[feature] * merged_data[macro_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_columns = ['DATE', 'permno', 'RET'] + characteristics + [f'{ch}_x_{mv}' for ch in characteristics for mv in ma_predictors]\n",
    "final_data = merged_data[final_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values in the dataset.\n"
     ]
    }
   ],
   "source": [
    "missing_values = final_data.isnull().sum()\n",
    "missing_columns = missing_values[missing_values > 0]\n",
    "\n",
    "if missing_columns.empty:\n",
    "    print(\"No missing values in the dataset.\")\n",
    "else:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['mve'] = final_data['mvel1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_convert = final_data.columns.difference(['DATE', 'RET'])\n",
    "\n",
    "final_data[cols_to_convert] = final_data[cols_to_convert].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>permno</th>\n",
       "      <th>RET</th>\n",
       "      <th>beta</th>\n",
       "      <th>turn</th>\n",
       "      <th>std_turn</th>\n",
       "      <th>mom6m</th>\n",
       "      <th>ill</th>\n",
       "      <th>agr</th>\n",
       "      <th>chmom</th>\n",
       "      <th>...</th>\n",
       "      <th>mom1m_x_svar</th>\n",
       "      <th>mom36m_x_dp_sp</th>\n",
       "      <th>mom36m_x_ep_sp</th>\n",
       "      <th>mom36m_x_bm_sp</th>\n",
       "      <th>mom36m_x_ntis</th>\n",
       "      <th>mom36m_x_tbl</th>\n",
       "      <th>mom36m_x_tms</th>\n",
       "      <th>mom36m_x_dfy</th>\n",
       "      <th>mom36m_x_svar</th>\n",
       "      <th>mve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>10001.0</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>-0.334353</td>\n",
       "      <td>-0.996730</td>\n",
       "      <td>-0.997841</td>\n",
       "      <td>-0.624620</td>\n",
       "      <td>-0.997955</td>\n",
       "      <td>0.738642</td>\n",
       "      <td>-0.016437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302977</td>\n",
       "      <td>-0.765574</td>\n",
       "      <td>-0.838730</td>\n",
       "      <td>-0.835249</td>\n",
       "      <td>0.235811</td>\n",
       "      <td>-0.868736</td>\n",
       "      <td>-0.826438</td>\n",
       "      <td>-0.692673</td>\n",
       "      <td>-0.623613</td>\n",
       "      <td>24355.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>10002.0</td>\n",
       "      <td>0.088435</td>\n",
       "      <td>-0.277169</td>\n",
       "      <td>-0.998115</td>\n",
       "      <td>-0.996156</td>\n",
       "      <td>-0.740385</td>\n",
       "      <td>-0.987881</td>\n",
       "      <td>0.757405</td>\n",
       "      <td>-0.053515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307640</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>-0.859870</td>\n",
       "      <td>-0.851483</td>\n",
       "      <td>0.240010</td>\n",
       "      <td>-0.907100</td>\n",
       "      <td>-0.831367</td>\n",
       "      <td>-0.714360</td>\n",
       "      <td>-0.633696</td>\n",
       "      <td>78332.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>10012.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.487780</td>\n",
       "      <td>-0.956415</td>\n",
       "      <td>-0.964540</td>\n",
       "      <td>-0.945455</td>\n",
       "      <td>-0.999823</td>\n",
       "      <td>0.677703</td>\n",
       "      <td>-0.248726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250735</td>\n",
       "      <td>-0.709595</td>\n",
       "      <td>-0.784748</td>\n",
       "      <td>-0.793793</td>\n",
       "      <td>0.225088</td>\n",
       "      <td>-0.770768</td>\n",
       "      <td>-0.813849</td>\n",
       "      <td>-0.637291</td>\n",
       "      <td>-0.597866</td>\n",
       "      <td>39836.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>10016.0</td>\n",
       "      <td>0.030726</td>\n",
       "      <td>-0.194900</td>\n",
       "      <td>-0.986370</td>\n",
       "      <td>-0.958998</td>\n",
       "      <td>-0.641981</td>\n",
       "      <td>-0.999895</td>\n",
       "      <td>0.770680</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319705</td>\n",
       "      <td>-0.733920</td>\n",
       "      <td>-0.808205</td>\n",
       "      <td>-0.811807</td>\n",
       "      <td>0.229748</td>\n",
       "      <td>-0.813338</td>\n",
       "      <td>-0.819319</td>\n",
       "      <td>-0.661356</td>\n",
       "      <td>-0.609054</td>\n",
       "      <td>379569.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>10019.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.413153</td>\n",
       "      <td>-0.974922</td>\n",
       "      <td>-0.918206</td>\n",
       "      <td>-0.769568</td>\n",
       "      <td>-0.999403</td>\n",
       "      <td>0.850155</td>\n",
       "      <td>-0.029024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278801</td>\n",
       "      <td>-0.798230</td>\n",
       "      <td>-0.870222</td>\n",
       "      <td>-0.859433</td>\n",
       "      <td>0.242066</td>\n",
       "      <td>-0.925888</td>\n",
       "      <td>-0.833781</td>\n",
       "      <td>-0.724981</td>\n",
       "      <td>-0.638634</td>\n",
       "      <td>28945.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE   permno       RET      beta      turn  std_turn     mom6m  \\\n",
       "0 2001-01-31  10001.0  0.012821 -0.334353 -0.996730 -0.997841 -0.624620   \n",
       "1 2001-01-31  10002.0  0.088435 -0.277169 -0.998115 -0.996156 -0.740385   \n",
       "2 2001-01-31  10012.0  0.500000  0.487780 -0.956415 -0.964540 -0.945455   \n",
       "3 2001-01-31  10016.0  0.030726 -0.194900 -0.986370 -0.958998 -0.641981   \n",
       "4 2001-01-31  10019.0  0.071429  0.413153 -0.974922 -0.918206 -0.769568   \n",
       "\n",
       "        ill       agr     chmom  ...  mom1m_x_svar  mom36m_x_dp_sp  \\\n",
       "0 -0.997955  0.738642 -0.016437  ...      0.302977       -0.765574   \n",
       "1 -0.987881  0.757405 -0.053515  ...      0.307640       -0.787495   \n",
       "2 -0.999823  0.677703 -0.248726  ...      0.250735       -0.709595   \n",
       "3 -0.999895  0.770680  0.001071  ...      0.319705       -0.733920   \n",
       "4 -0.999403  0.850155 -0.029024  ...      0.278801       -0.798230   \n",
       "\n",
       "   mom36m_x_ep_sp  mom36m_x_bm_sp  mom36m_x_ntis  mom36m_x_tbl  mom36m_x_tms  \\\n",
       "0       -0.838730       -0.835249       0.235811     -0.868736     -0.826438   \n",
       "1       -0.859870       -0.851483       0.240010     -0.907100     -0.831367   \n",
       "2       -0.784748       -0.793793       0.225088     -0.770768     -0.813849   \n",
       "3       -0.808205       -0.811807       0.229748     -0.813338     -0.819319   \n",
       "4       -0.870222       -0.859433       0.242066     -0.925888     -0.833781   \n",
       "\n",
       "   mom36m_x_dfy  mom36m_x_svar         mve  \n",
       "0     -0.692673      -0.623613   24355.500  \n",
       "1     -0.714360      -0.633696   78332.625  \n",
       "2     -0.637291      -0.597866   39836.000  \n",
       "3     -0.661356      -0.609054  379569.500  \n",
       "4     -0.724981      -0.638634   28945.000  \n",
       "\n",
       "[5 rows x 211 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler with feature range between -1 and 1\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# List of columns to exclude from scaling\n",
    "exclude_columns = ['DATE', 'permno', 'RET','mve']\n",
    "\n",
    "# Select the columns that need to be scaled\n",
    "columns_to_scale = final_data.columns.difference(exclude_columns)\n",
    "\n",
    "# Apply the scaler to the selected columns\n",
    "final_data[columns_to_scale] = scaler.fit_transform(final_data[columns_to_scale])\n",
    "\n",
    "# Display the scaled data\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_top = final_data.sort_values('mve',ascending=False).groupby('DATE').head(1000).reset_index(drop=True)\n",
    "#final_data_bot = final_data.sort_values('mve',ascending=False).groupby('DATE').tail(1000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del([ch,characteristics,columns_to_scale,data_ch,data_ma,exclude_columns,feature,final_columns,interaction_term_name,ma_predictors,macro_var,merged_data,missing_columns,missing_values,nddt,scaler,stdt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(cols_to_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(final_data)\n",
    "data_top = final_data_top.copy()\n",
    "del(final_data_top)\n",
    "del(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_top = data_top.sort_values(by=['DATE', 'permno']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(data_top.columns).difference({'permno','DATE','mve','RET'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split the datasets into train, valid and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time frontiers for validation and test sets\n",
    "stdt_vld = np.datetime64('2009-01-31')\n",
    "stdt_tst = np.datetime64('2013-01-31')\n",
    "\n",
    "# Training set: data before the validation start date\n",
    "train_data = data_top[data_top['DATE'] < stdt_vld]\n",
    "\n",
    "# Validation set: data between validation start date and test start date\n",
    "valid_data = data_top[(data_top['DATE'] >= stdt_vld) & (data_top['DATE'] < stdt_tst)]\n",
    "\n",
    "# Test set: data after the test start date\n",
    "test_data = data_top[data_top['DATE'] >= stdt_tst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model implementation\n",
    "## Performance metrics\n",
    "\n",
    "### Define the $R_{oos}^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Scoring Function\n",
    "# out-of-sample R squared\n",
    "def R_oos(actual, predicted):\n",
    "    actual, predicted = np.array(actual), np.array(predicted).flatten()\n",
    "    #predicted = np.where(predicted<0,0,predicted)\n",
    "    return 1 - (np.dot((actual-predicted),(actual-predicted)))/(np.dot(actual,actual))\n",
    "\n",
    "# Evaluation Output\n",
    "def evaluate(actual, predicted):\n",
    "    print('*'*15+'Out-of-Sample Metrics'+'*'*15)\n",
    "    print(f'The out-of-sample R2 is {R_oos(actual,predicted)*100:.2f}%')\n",
    "    print(f'The out-of-sample MSE is {mean_squared_error(actual,predicted):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS\n",
    "### All features\n",
    "Using the $l_2$ as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -68.87%\n",
      "The out-of-sample MSE is 0.011\n",
      "CPU times: total: 16.4 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "OLS = LinearRegression().fit(train_data[features],train_data['RET'])\n",
    "evaluate(test_data['RET'], OLS.predict(test_data[features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Huber loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -0.38%\n",
      "The out-of-sample MSE is 0.006\n",
      "CPU times: total: 1min 57s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time.sleep(10)\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "epsilon = np.max(((train_data['RET']-OLS.predict(train_data[features])).quantile(.999),1))\n",
    "OLS_H = HuberRegressor(epsilon=epsilon).fit(train_data[features],train_data['RET'])\n",
    "evaluate(test_data['RET'], OLS_H.predict(test_data[features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3 = ['mvel1','bm','mom1m','mom6m','mom12m','mom36m']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $l_2$ as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -0.59%\n",
      "The out-of-sample MSE is 0.006\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 47.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "OLS_3 = LinearRegression().fit(train_data[features_3],train_data['RET'])\n",
    "evaluate(test_data['RET'], OLS_3.predict(test_data[features_3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Huber loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.82%\n",
      "The out-of-sample MSE is 0.006\n",
      "CPU times: total: 2.62 s\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "epsilon = np.max(((train_data['RET']-OLS_3.predict(train_data[features_3])).quantile(.999),1))\n",
    "OLS_H_3 = HuberRegressor(epsilon=epsilon).fit(train_data[features_3],train_data['RET'])\n",
    "evaluate(test_data['RET'], OLS_H_3.predict(test_data[features_3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'pls__n_components': 5}\n",
      "Best R_oos: 4.62%\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 4.79%\n",
      "The out-of-sample MSE is 0.006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Define the PLS pipeline creation function\n",
    "def create_pls_pipeline(n_components):\n",
    "    return Pipeline([\n",
    "        ('pls', PLSRegression(n_components=n_components))  # PLS for regression\n",
    "    ])\n",
    "\n",
    "# Define the parameter grid for PLS components\n",
    "params = {'pls__n_components': [1,2,3,4,5]}\n",
    "\n",
    "# Validation function for PLS\n",
    "def val_fun_pls(params, X_trn, y_trn, X_vld, y_vld):\n",
    "    best_ros = -float('inf')\n",
    "    best_mod = None\n",
    "    best_param = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    \n",
    "    for param in lst_params:\n",
    "        mod = create_pls_pipeline(n_components=param['pls__n_components'])\n",
    "        mod.fit(X_trn, y_trn)\n",
    "        y_pred = mod.predict(X_vld)\n",
    "        ros = R_oos(y_vld, y_pred)\n",
    "        \n",
    "        if ros > best_ros:\n",
    "            best_ros = ros\n",
    "            best_mod = mod\n",
    "            best_param = param\n",
    "\n",
    "    print(f'Best parameters: {best_param}')\n",
    "    print(f'Best R_oos: {best_ros*100:.2f}%')\n",
    "    return best_mod\n",
    "\n",
    "# Perform hyperparameter tuning for PLS\n",
    "PLS_best_model = val_fun_pls(params=params, X_trn=train_data[features], y_trn=train_data['RET'], X_vld=valid_data[features], y_vld=valid_data['RET'])\n",
    "\n",
    "# Predict using the optimal PLS model on the test set\n",
    "pls_pred_os = PLS_best_model.predict(test_data[features])\n",
    "\n",
    "# Evaluate the PLS model's performance\n",
    "evaluate(test_data['RET'], pls_pred_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'pca__n_components': 38}\n",
      "Best R_oos: 5.00%\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 4.78%\n",
      "The out-of-sample MSE is 0.006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the PCR pipeline creation function\n",
    "def create_pcr_pipeline(n_components):\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Standardize features\n",
    "        ('pca', PCA(n_components=n_components)),  # PCA for dimensionality reduction\n",
    "        ('regression', LinearRegression())  # Linear regression on principal components\n",
    "    ])\n",
    "\n",
    "# Define the parameter grid for PCA components\n",
    "params = {'pca__n_components': [36,37,38,39,40]}\n",
    "\n",
    "# Validation function for PCR\n",
    "def val_fun_pcr(params, X_trn, y_trn, X_vld, y_vld):\n",
    "    best_ros = -float('inf')\n",
    "    best_mod = None\n",
    "    best_param = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    \n",
    "    for param in lst_params:\n",
    "        mod = create_pcr_pipeline(n_components=param['pca__n_components'])\n",
    "        mod.fit(X_trn, y_trn)\n",
    "        y_pred = mod.predict(X_vld)\n",
    "        ros = R_oos(y_vld, y_pred)\n",
    "        \n",
    "        if ros > best_ros:\n",
    "            best_ros = ros\n",
    "            best_mod = mod\n",
    "            best_param = param\n",
    "\n",
    "    print(f'Best parameters: {best_param}')\n",
    "    print(f'Best R_oos: {best_ros*100:.2f}%')\n",
    "    return best_mod\n",
    "\n",
    "# Perform hyperparameter tuning for PCR\n",
    "PCR = val_fun_pcr(params=params, X_trn=train_data[features], y_trn=train_data['RET'], X_vld=valid_data[features], y_vld=valid_data['RET'])\n",
    "\n",
    "# Predict using the optimal PCR model on the test set\n",
    "pcr_pred_os = PCR.predict(test_data[features])\n",
    "\n",
    "# Evaluate the PCR model's performance\n",
    "def evaluate(actual, predicted):\n",
    "    print('*'*15 + 'Out-of-Sample Metrics' + '*'*15)\n",
    "    print(f'The out-of-sample R2 is {R_oos(actual, predicted)*100:.2f}%')\n",
    "    print(f'The out-of-sample MSE is {mean_squared_error(actual, predicted):.3f}')\n",
    "\n",
    "evaluate(test_data['RET'], pcr_pred_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "First, import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn1_model(input_dim, hidden_units):\n",
    "    model = Sequential()\n",
    "    # Input layer with hidden units\n",
    "    model.add(Dense(hidden_units, input_dim=input_dim, activation='relu'))\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilation and training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn1_model(X_trn, y_trn, X_vld, y_vld, hidden_units, batch_size=32, epochs=100):\n",
    "    model = create_nn1_model(X_trn.shape[1], hidden_units)\n",
    "    \n",
    "    # Compile the model with Adam optimizer\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_trn, y_trn, \n",
    "                        validation_data=(X_vld, y_vld),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=1)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn1_model(model, X_tst, y_tst):\n",
    "    y_pred = model.predict(X_tst).flatten()\n",
    "    print('*'*15 + 'Out-of-Sample Metrics' + '*'*15)\n",
    "    print(f'The out-of-sample R2 is {R_oos(y_tst, y_pred)*100:.2f}%')\n",
    "    print(f'The out-of-sample MSE is {mean_squared_error(y_tst, y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define architecture parameters\n",
    "hidden_units = 32  # Number of neurons in the hidden layer\n",
    "\n",
    "# Train the model\n",
    "nn1_model, history = train_nn1_model(X_trn, y_trn, X_vld, y_vld, hidden_units)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_nn1_model(nn1_model, X_tst, y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn2_model(input_dim, hidden_units1, hidden_units2):\n",
    "    model = Sequential()\n",
    "    # First hidden layer\n",
    "    model.add(Dense(hidden_units1, input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    # Second hidden layer\n",
    "    model.add(Dense(hidden_units2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn2_model(X_trn, y_trn, X_vld, y_vld, hidden_units1, hidden_units2, batch_size=32, epochs=100):\n",
    "    model = create_nn2_model(X_trn.shape[1], hidden_units1, hidden_units2)\n",
    "    \n",
    "    # Compile the model with Adam optimizer\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_trn, y_trn, \n",
    "                        validation_data=(X_vld, y_vld),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=1)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn2_model(model, X_tst, y_tst):\n",
    "    y_pred = model.predict(X_tst).flatten()\n",
    "    print('*'*15 + 'Out-of-Sample Metrics' + '*'*15)\n",
    "    print(f'The out-of-sample R2 is {R_oos(y_tst, y_pred)*100:.2f}%')\n",
    "    print(f'The out-of-sample MSE is {mean_squared_error(y_tst, y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define architecture parameters\n",
    "hidden_units1 = 32  # Number of neurons in the first hidden layer\n",
    "hidden_units2 = 16  # Number of neurons in the second hidden layer\n",
    "\n",
    "# Train the model\n",
    "nn2_model, history = train_nn2_model(X_trn, y_trn, X_vld, y_vld, hidden_units1, hidden_units2)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_nn2_model(nn2_model, X_tst, y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn3_model(input_dim, hidden_units1, hidden_units2, hidden_units3):\n",
    "    model = Sequential()\n",
    "    # First hidden layer\n",
    "    model.add(Dense(hidden_units1, input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    # Second hidden layer\n",
    "    model.add(Dense(hidden_units2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    # Third hidden layer\n",
    "    model.add(Dense(hidden_units3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn3_model(X_trn, y_trn, X_vld, y_vld, hidden_units1, hidden_units2, hidden_units3, batch_size=32, epochs=100):\n",
    "    model = create_nn3_model(X_trn.shape[1], hidden_units1, hidden_units2, hidden_units3)\n",
    "    \n",
    "    # Compile the model with Adam optimizer\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_trn, y_trn, \n",
    "                        validation_data=(X_vld, y_vld),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=1)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn3_model(model, X_tst, y_tst):\n",
    "    y_pred = model.predict(X_tst).flatten()\n",
    "    print('*'*15 + 'Out-of-Sample Metrics' + '*'*15)\n",
    "    print(f'The out-of-sample R2 is {R_oos(y_tst, y_pred)*100:.2f}%')\n",
    "    print(f'The out-of-sample MSE is {mean_squared_error(y_tst, y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define architecture parameters\n",
    "hidden_units1 = 32  # Number of neurons in the first hidden layer\n",
    "hidden_units2 = 16  # Number of neurons in the second hidden layer\n",
    "hidden_units3 = 8   # Number of neurons in the third hidden layer\n",
    "\n",
    "# Train the model\n",
    "nn3_model, history = train_nn3_model(X_trn, y_trn, X_vld, y_vld, hidden_units1, hidden_units2, hidden_units3)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_nn3_model(nn3_model, X_tst, y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN + Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn1_model(input_dim, hidden_units):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units, input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    return model\n",
    "\n",
    "def train_nn1_model(X_trn, y_trn, X_vld, y_vld, hidden_units=32, batch_size=32, epochs=100):\n",
    "    model = create_nn1_model(X_trn.shape[1], hidden_units)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_trn, y_trn, \n",
    "                        validation_data=(X_vld, y_vld),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=1)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 0.2620 - val_loss: 0.0110\n",
      "Epoch 2/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 3/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 4/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 5/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 6/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 7/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 8/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0105 - val_loss: 0.0119\n",
      "Epoch 9/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0103 - val_loss: 0.0156\n",
      "Epoch 10/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0104 - val_loss: 0.0123\n",
      "Epoch 11/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0105 - val_loss: 0.0172\n",
      "Epoch 12/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0103 - val_loss: 0.0139\n",
      "Epoch 13/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0105 - val_loss: 0.0140\n",
      "Epoch 14/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0104 - val_loss: 0.0133\n",
      "Epoch 15/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0103 - val_loss: 0.0160\n",
      "\u001b[1m2625/2625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 1.90%\n",
      "The out-of-sample MSE is 0.006\n"
     ]
    }
   ],
   "source": [
    "# Train the NN1 model\n",
    "hidden_units = 32  # Number of neurons in the hidden layer\n",
    "NN1_model, NN1_history = train_nn1_model(train_data[features], train_data['RET'], valid_data[features], valid_data['RET'], hidden_units=hidden_units)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "NN1_predictions = NN1_model.predict(test_data[features])\n",
    "evaluate(test_data['RET'], NN1_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn2_model(input_dim, hidden_units1, hidden_units2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units1, input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(hidden_units2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(1))  # Output layer\n",
    "    return model\n",
    "\n",
    "def train_nn2_model(X_trn, y_trn, X_vld, y_vld, hidden_units1=32, hidden_units2=16, batch_size=32, epochs=100):\n",
    "    model = create_nn2_model(X_trn.shape[1], hidden_units1, hidden_units2)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_trn, y_trn, \n",
    "                        validation_data=(X_vld, y_vld),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=1)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - loss: 0.6400 - val_loss: 0.0106\n",
      "Epoch 2/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 3/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 4/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 0.0116 - val_loss: 0.0123\n",
      "Epoch 5/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 6/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 7/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 8/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 9/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 10/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 11/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0103 - val_loss: 0.0122\n",
      "Epoch 12/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0104 - val_loss: 0.0129\n",
      "Epoch 13/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0103 - val_loss: 0.0155\n",
      "Epoch 14/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 15/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "\u001b[1m2625/2625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 2.46%\n",
      "The out-of-sample MSE is 0.006\n"
     ]
    }
   ],
   "source": [
    "# Train the NN2 model\n",
    "hidden_units1 = 32  # Number of neurons in the first hidden layer\n",
    "hidden_units2 = 16  # Number of neurons in the second hidden layer\n",
    "NN2_model, NN2_history = train_nn2_model(train_data[features], train_data['RET'], valid_data[features], valid_data['RET'], hidden_units1=hidden_units1, hidden_units2=hidden_units2)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "NN2_predictions = NN2_model.predict(test_data[features])\n",
    "evaluate(test_data['RET'], NN2_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn3_model(input_dim, hidden_units1, hidden_units2, hidden_units3):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units1, input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(hidden_units2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(hidden_units3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(1))  # Output layer\n",
    "    return model\n",
    "\n",
    "def train_nn3_model(X_trn, y_trn, X_vld, y_vld, hidden_units1=32, hidden_units2=16, hidden_units3=8, batch_size=32, epochs=100):\n",
    "    model = create_nn3_model(X_trn.shape[1], hidden_units1, hidden_units2, hidden_units3)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_trn, y_trn, \n",
    "                        validation_data=(X_vld, y_vld),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=1)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - loss: 0.8721 - val_loss: 0.0105\n",
      "Epoch 2/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 3/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 4/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 5/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0112 - val_loss: 0.0103\n",
      "Epoch 6/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 7/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0109 - val_loss: 0.0136\n",
      "Epoch 8/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 9/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 10/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 11/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.0105 - val_loss: 0.0107\n",
      "Epoch 12/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 13/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 14/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.0106 - val_loss: 0.0129\n",
      "Epoch 15/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 16/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 17/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 18/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.0106 - val_loss: 0.0122\n",
      "\u001b[1m2625/2625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 4.14%\n",
      "The out-of-sample MSE is 0.006\n"
     ]
    }
   ],
   "source": [
    "# Train the NN3 model\n",
    "hidden_units1 = 32  # Number of neurons in the first hidden layer\n",
    "hidden_units2 = 16  # Number of neurons in the second hidden layer\n",
    "hidden_units3 = 8   # Number of neurons in the third hidden layer\n",
    "NN3_model, NN3_history = train_nn3_model(train_data[features], train_data['RET'], valid_data[features], valid_data['RET'], hidden_units1=hidden_units1, hidden_units2=hidden_units2, hidden_units3=hidden_units3)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "NN3_predictions = NN3_model.predict(test_data[features])\n",
    "evaluate(test_data['RET'], NN3_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn4_model(input_dim, hidden_units1, hidden_units2, hidden_units3, hidden_units4):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units1, input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(hidden_units2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(hidden_units3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(hidden_units4, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(1))  # Output layer\n",
    "    return model\n",
    "\n",
    "def train_nn4_model(X_trn, y_trn, X_vld, y_vld, hidden_units1=32, hidden_units2=16, hidden_units3=8, hidden_units4=4, batch_size=32, epochs=100):\n",
    "    model = create_nn4_model(X_trn.shape[1], hidden_units1, hidden_units2, hidden_units3, hidden_units4)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_trn, y_trn, \n",
    "                        validation_data=(X_vld, y_vld),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=1)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 0.5217 - val_loss: 0.0104\n",
      "Epoch 2/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0128 - val_loss: 0.0102\n",
      "Epoch 3/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0115 - val_loss: 0.0102\n",
      "Epoch 4/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 5/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0116 - val_loss: 0.0106\n",
      "Epoch 6/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0112 - val_loss: 0.0107\n",
      "Epoch 7/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 8/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 9/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 10/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 11/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 12/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 13/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0108 - val_loss: 0.0115\n",
      "Epoch 14/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 15/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 16/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0109 - val_loss: 0.0117\n",
      "Epoch 17/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.0107 - val_loss: 0.0109\n",
      "\u001b[1m2625/2625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 2.30%\n",
      "The out-of-sample MSE is 0.006\n"
     ]
    }
   ],
   "source": [
    "# Train the NN4 model\n",
    "hidden_units1 = 32  # Number of neurons in the first hidden layer\n",
    "hidden_units2 = 16  # Number of neurons in the second hidden layer\n",
    "hidden_units3 = 8   # Number of neurons in the third hidden layer\n",
    "hidden_units4 = 4   # Number of neurons in the fourth hidden layer\n",
    "NN4_model, NN4_history = train_nn4_model(train_data[features], train_data['RET'], valid_data[features], valid_data['RET'], hidden_units1=hidden_units1, hidden_units2=hidden_units2, hidden_units3=hidden_units3, hidden_units4=hidden_units4)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "NN4_predictions = NN4_model.predict(test_data[features])\n",
    "evaluate(test_data['RET'], NN4_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn5_model(input_dim, hidden_units1, hidden_units2, hidden_units3, hidden_units4, hidden_units5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units1, input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(hidden_units2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(hidden_units3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(hidden_units4, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(hidden_units5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(1))  # Output layer\n",
    "    return model\n",
    "\n",
    "def train_nn5_model(X_trn, y_trn, X_vld, y_vld, hidden_units1=32, hidden_units2=16, hidden_units3=8, hidden_units4=4, hidden_units5=2, batch_size=32, epochs=100):\n",
    "    model = create_nn5_model(X_trn.shape[1], hidden_units1, hidden_units2, hidden_units3, hidden_units4, hidden_units5)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_trn, y_trn, \n",
    "                        validation_data=(X_vld, y_vld),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=1)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 6ms/step - loss: 1.6171 - val_loss: 0.0115\n",
      "Epoch 2/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.0175 - val_loss: 0.0104\n",
      "Epoch 3/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 4/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 5/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - loss: 0.0116 - val_loss: 0.0102\n",
      "Epoch 6/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - loss: 0.0118 - val_loss: 0.0102\n",
      "Epoch 7/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.0116 - val_loss: 0.0105\n",
      "Epoch 8/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - loss: 0.0114 - val_loss: 0.0102\n",
      "Epoch 9/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 10/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.0113 - val_loss: 0.0103\n",
      "Epoch 11/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 12/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 13/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 14/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 15/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 16/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.0111 - val_loss: 0.0124\n",
      "Epoch 17/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 18/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 19/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 20/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 21/100\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.0108 - val_loss: 0.0104\n",
      "\u001b[1m2625/2625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 2.66%\n",
      "The out-of-sample MSE is 0.006\n"
     ]
    }
   ],
   "source": [
    "# Train the NN5 model\n",
    "hidden_units1 = 32  # Number of neurons in the first hidden layer\n",
    "hidden_units2 = 16  # Number of neurons in the second hidden layer\n",
    "hidden_units3 = 8   # Number of neurons in the third hidden layer\n",
    "hidden_units4 = 4   # Number of neurons in the fourth hidden layer\n",
    "hidden_units5 = 2   # Number of neurons in the fifth hidden layer\n",
    "NN5_model, NN5_history = train_nn5_model(train_data[features], train_data['RET'], valid_data[features], valid_data['RET'], hidden_units1=hidden_units1, hidden_units2=hidden_units2, hidden_units3=hidden_units3, hidden_units4=hidden_units4, hidden_units5=hidden_units5)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "NN5_predictions = NN5_model.predict(test_data[features])\n",
    "evaluate(test_data['RET'], NN5_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolios\n",
    "## Prespecified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the top 500 firms by 'mve' for each 'DATE'\n",
    "top_500_data = test_data.sort_values('mve', ascending=False).groupby('DATE').head(500).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate the weights for each firm based on 'mve'\n",
    "top_500_data['weight'] = top_500_data.groupby('DATE')['mve'].transform(lambda x: x / x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict using the OLS model\n",
    "top_500_data['OLS_pred'] = OLS_3.predict(top_500_data[features_3])\n",
    "\n",
    "# Predict using the Huber model\n",
    "top_500_data['Huber_pred'] = OLS_H_3.predict(top_500_data[features_3])\n",
    "\n",
    "# Predict using the PLS model\n",
    "top_500_data['PLS_pred'] = PLS_best_model.predict(top_500_data[features])\n",
    "\n",
    "# Predict using the PCR model\n",
    "top_500_data['PCR_pred'] = PCR.predict(top_500_data[features])\n",
    "\n",
    "# Predict using the NN\n",
    "top_500_data['NN1_pred'] = NN1_model.predict(top_500_data[features])\n",
    "top_500_data['NN2_pred'] = NN2_model.predict(top_500_data[features])\n",
    "top_500_data['NN3_pred'] = NN3_model.predict(top_500_data[features])\n",
    "top_500_data['NN4_pred'] = NN4_model.predict(top_500_data[features])\n",
    "top_500_data['NN5_pred'] = NN5_model.predict(top_500_data[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_returns = top_500_data.groupby('DATE').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'RET': np.average(x['RET'], weights=x['weight']),\n",
    "        'OLS_pred': np.average(x['OLS_pred'], weights=x['weight']),\n",
    "        'Huber_pred': np.average(x['Huber_pred'], weights=x['weight']),\n",
    "        'PLS_pred': np.average(x['PLS_pred'], weights=x['weight']),\n",
    "        'PCR_pred': np.average(x['PCR_pred'], weights=x['weight']),\n",
    "        'NN1_pred': np.average(x['NN1_pred'], weights=x['weight']),\n",
    "        'NN2_pred': np.average(x['NN2_pred'], weights=x['weight']),\n",
    "        'NN3_pred': np.average(x['NN3_pred'], weights=x['weight']),\n",
    "        'NN4_pred': np.average(x['NN4_pred'], weights=x['weight']),\n",
    "        'NN5_pred': np.average(x['NN5_pred'], weights=x['weight']),\n",
    "    })\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************R_oos for Models Replicating S&P 500***************\n",
      "OLS_pred: R_oos = -12.23%\n",
      "Huber_pred: R_oos = 1.05%\n",
      "PLS_pred: R_oos = 33.33%\n",
      "PCR_pred: R_oos = 29.81%\n",
      "NN1_pred: R_oos = 24.76%\n",
      "NN2_pred: R_oos = 10.53%\n",
      "NN3_pred: R_oos = 21.78%\n",
      "NN4_pred: R_oos = 14.66%\n",
      "NN5_pred: R_oos = 16.41%\n"
     ]
    }
   ],
   "source": [
    "# Define the R_oos function\n",
    "def R_oos_sp500(actual, predicted):\n",
    "    actual, predicted = np.array(actual), np.array(predicted).flatten()\n",
    "    return 1 - (np.dot((actual - predicted), (actual - predicted))) / (np.dot(actual, actual))\n",
    "\n",
    "# Evaluate for each model\n",
    "print('*' * 15 + 'R_oos for Models Replicating S&P 500' + '*' * 15)\n",
    "for model in ['OLS_pred', 'Huber_pred', 'PLS_pred', 'PCR_pred','NN1_pred','NN2_pred','NN3_pred','NN4_pred','NN5_pred']:\n",
    "    r_oos_value = R_oos_sp500(monthly_returns['RET'], monthly_returns[model])\n",
    "    print(f'{model}: R_oos = {r_oos_value * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2625/2625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n",
      "\u001b[1m2625/2625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "\u001b[1m2625/2625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "\u001b[1m2625/2625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "\u001b[1m2625/2625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate predictions for each model\n",
    "test_data['OLS_pred'] = OLS_3.predict(test_data[features_3])\n",
    "test_data['Huber_pred'] = OLS_H_3.predict(test_data[features_3])\n",
    "test_data['PLS_pred'] = PLS_best_model.predict(test_data[features])\n",
    "test_data['PCR_pred'] = PCR.predict(test_data[features])\n",
    "test_data['NN1_pred'] = NN1_model.predict(test_data[features])\n",
    "test_data['NN2_pred'] = NN2_model.predict(test_data[features])\n",
    "test_data['NN3_pred'] = NN3_model.predict(test_data[features])\n",
    "test_data['NN4_pred'] = NN4_model.predict(test_data[features])\n",
    "test_data['NN5_pred'] = NN5_model.predict(test_data[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Sort stocks into deciles based on predicted returns\n",
    "def assign_deciles(pred_col):\n",
    "    test_data['decile'] = test_data.groupby('DATE')[pred_col].transform(\n",
    "        lambda x: pd.qcut(x, 10, labels=False, duplicates='drop') + 1\n",
    "    )\n",
    "\n",
    "# Assign deciles based on predictions for each model\n",
    "assign_deciles('OLS_pred')\n",
    "test_data['OLS_decile'] = test_data['decile']\n",
    "\n",
    "assign_deciles('Huber_pred')\n",
    "test_data['Huber_decile'] = test_data['decile']\n",
    "\n",
    "assign_deciles('PLS_pred')\n",
    "test_data['PLS_decile'] = test_data['decile']\n",
    "\n",
    "assign_deciles('PCR_pred')\n",
    "test_data['PCR_decile'] = test_data['decile']\n",
    "\n",
    "assign_deciles('NN1_pred')\n",
    "test_data['NN1_decile'] = test_data['decile']\n",
    "\n",
    "assign_deciles('NN2_pred')\n",
    "test_data['NN2_decile'] = test_data['decile']\n",
    "\n",
    "assign_deciles('NN3_pred')\n",
    "test_data['NN3_decile'] = test_data['decile']\n",
    "\n",
    "assign_deciles('NN4_pred')\n",
    "test_data['NN4_decile'] = test_data['decile']\n",
    "\n",
    "assign_deciles('NN5_pred')\n",
    "test_data['NN5_decile'] = test_data['decile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate simple average returns for each decile and model\n",
    "def calculate_simple_average_return(data, decile_col):\n",
    "    # Group by DATE and decile, then calculate mean return for each decile within each month\n",
    "    return data.groupby(['DATE', decile_col])['RET'].mean().unstack()\n",
    "\n",
    "# Calculate simple average returns for each decile and model\n",
    "ols_deciles_returns = calculate_simple_average_return(test_data, 'OLS_decile')\n",
    "huber_deciles_returns = calculate_simple_average_return(test_data, 'Huber_decile')\n",
    "pls_deciles_returns = calculate_simple_average_return(test_data, 'PLS_decile')\n",
    "pcr_deciles_returns = calculate_simple_average_return(test_data, 'PCR_decile')\n",
    "nn1_deciles_returns = calculate_simple_average_return(test_data, 'NN1_decile')\n",
    "nn2_deciles_returns = calculate_simple_average_return(test_data, 'NN2_decile')\n",
    "nn3_deciles_returns = calculate_simple_average_return(test_data, 'NN3_decile')\n",
    "nn4_deciles_returns = calculate_simple_average_return(test_data, 'NN4_decile')\n",
    "nn5_deciles_returns = calculate_simple_average_return(test_data, 'NN5_decile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Construct the Zero-Net-Investment portfolios\n",
    "def calculate_zni_returns(deciles_returns):\n",
    "    # Calculate the difference between the top and bottom decile returns\n",
    "    return deciles_returns[10] - deciles_returns[1]\n",
    "\n",
    "# Calculate ZNI portfolio returns for each model\n",
    "ols_zni_returns = calculate_zni_returns(ols_deciles_returns)\n",
    "huber_zni_returns = calculate_zni_returns(huber_deciles_returns)\n",
    "pls_zni_returns = calculate_zni_returns(pls_deciles_returns)\n",
    "pcr_zni_returns = calculate_zni_returns(pcr_deciles_returns)\n",
    "nn1_zni_returns = calculate_zni_returns(nn1_deciles_returns)\n",
    "nn2_zni_returns = calculate_zni_returns(nn2_deciles_returns)\n",
    "nn3_zni_returns = calculate_zni_returns(nn3_deciles_returns)\n",
    "nn4_zni_returns = calculate_zni_returns(nn4_deciles_returns)\n",
    "nn5_zni_returns = calculate_zni_returns(nn5_deciles_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Combine and evaluate ZNI portfolios\n",
    "zni_portfolios = pd.DataFrame({\n",
    "    'OLS_ZNI': ols_zni_returns,\n",
    "    'Huber_ZNI': huber_zni_returns,\n",
    "    'PLS_ZNI': pls_zni_returns,\n",
    "    'PCR_ZNI': pcr_zni_returns,\n",
    "    'NN1_ZNI': nn1_zni_returns,\n",
    "    'NN2_ZNI': nn2_zni_returns,\n",
    "    'NN3_ZNI': nn3_zni_returns,\n",
    "    'NN4_ZNI': nn4_zni_returns,\n",
    "    'NN5_ZNI': nn5_zni_returns\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path\n",
    "directory_path = r\"C:\\Users\\frank\\OneDrive - Escuela Superior de Economia y Negocios\\2. SS 2024\\ML Seminar\\data\\Clean\"\n",
    "\n",
    "# Define the file paths\n",
    "zni_portfolios_path = os.path.join(directory_path, 'zni_portfolios.csv')\n",
    "monthly_returns_path = os.path.join(directory_path, 'monthly_returns.csv')\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "zni_portfolios.to_csv(zni_portfolios_path, index=True)  # index=True to include the index in the CSV\n",
    "monthly_returns.to_csv(monthly_returns_path, index=True)  # index=True to include the index in the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        DATE   OLS_ZNI  Huber_ZNI   PLS_ZNI   PCR_ZNI   NN1_ZNI   NN2_ZNI  \\\n",
      "0 2013-01-31  0.016302   0.024629  0.027263 -0.005990  0.019370 -0.003016   \n",
      "1 2013-02-28  0.036944   0.029284 -0.008795  0.016225 -0.040705 -0.016622   \n",
      "2 2013-03-31  0.039756   0.032568 -0.010146 -0.023190 -0.029756 -0.041115   \n",
      "3 2013-04-30 -0.016158  -0.018293 -0.030515 -0.018367 -0.038761 -0.016134   \n",
      "4 2013-05-31  0.001337   0.001451  0.034203 -0.005684  0.061622  0.042626   \n",
      "\n",
      "    NN3_ZNI   NN4_ZNI   NN5_ZNI  SP500_RET  \n",
      "0  0.018499  0.006361  0.022363   0.058423  \n",
      "1 -0.029221  0.019581 -0.027901   0.006047  \n",
      "2 -0.042903 -0.025832 -0.031605   0.038767  \n",
      "3 -0.033762 -0.008265 -0.039524   0.018049  \n",
      "4  0.069833  0.017463  0.042039   0.018000  \n"
     ]
    }
   ],
   "source": [
    "# Ensure `monthly_returns` has a 'DATE' column and 'RET' column\n",
    "monthly_returns = monthly_returns.reset_index()  # Reset index to ensure 'DATE' is a column\n",
    "monthly_returns.rename(columns={'RET': 'SP500_RET'}, inplace=True)\n",
    "\n",
    "# Merge `monthly_returns` into `zni_portfolios` on 'DATE'\n",
    "zni_portfolios = zni_portfolios.reset_index()  # Reset index to ensure 'DATE' is a column\n",
    "merged_portfolios = pd.merge(zni_portfolios, monthly_returns[['DATE', 'SP500_RET']], on='DATE', how='left')\n",
    "\n",
    "print(merged_portfolios.head())  # Display the first few rows for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Sharpe Ratio for each portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratios:\n",
      "OLS_ZNI: -0.06\n",
      "Huber_ZNI: -0.07\n",
      "PLS_ZNI: 0.06\n",
      "PCR_ZNI: 0.14\n",
      "NN1_ZNI: -0.04\n",
      "NN2_ZNI: 0.15\n",
      "NN3_ZNI: 0.09\n",
      "NN4_ZNI: 0.12\n",
      "NN5_ZNI: -0.01\n",
      "SP500: 0.35\n"
     ]
    }
   ],
   "source": [
    "def calculate_sharpe_ratio(df, column_name):\n",
    "    \"\"\"Calculate the Sharpe Ratio for a given column in the DataFrame.\"\"\"\n",
    "    mean_return = df[column_name].mean()\n",
    "    std_dev = df[column_name].std()\n",
    "    return mean_return / std_dev if std_dev != 0 else np.nan\n",
    "\n",
    "# Calculate Sharpe Ratio for each portfolio\n",
    "sharpe_ratios = {col: calculate_sharpe_ratio(merged_portfolios, col) for col in merged_portfolios.columns if col.endswith('_ZNI')}\n",
    "\n",
    "# Calculate Sharpe Ratio for S&P500\n",
    "sharpe_ratios['SP500'] = calculate_sharpe_ratio(merged_portfolios, 'SP500_RET')\n",
    "\n",
    "# Print the Sharpe Ratios\n",
    "print(\"Sharpe Ratios:\")\n",
    "for portfolio, ratio in sharpe_ratios.items():\n",
    "    print(f\"{portfolio}: {ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and test $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpha and Beta:\n",
      "OLS_ZNI: Alpha = -0.0003, Beta = -0.1353, Alpha p-value = 0.9206 (Not Significant)\n",
      "Huber_ZNI: Alpha = -0.0008, Beta = -0.0858, Alpha p-value = 0.7801 (Not Significant)\n",
      "PLS_ZNI: Alpha = 0.0030, Beta = -0.0927, Alpha p-value = 0.3975 (Not Significant)\n",
      "PCR_ZNI: Alpha = 0.0052, Beta = -0.1560, Alpha p-value = 0.0796 (Not Significant)\n",
      "NN1_ZNI: Alpha = -0.0036, Beta = 0.2094, Alpha p-value = 0.3659 (Not Significant)\n",
      "NN2_ZNI: Alpha = 0.0044, Beta = 0.0111, Alpha p-value = 0.2235 (Not Significant)\n",
      "NN3_ZNI: Alpha = 0.0025, Beta = 0.0683, Alpha p-value = 0.5676 (Not Significant)\n",
      "NN4_ZNI: Alpha = 0.0060, Beta = -0.2605, Alpha p-value = 0.0423 (Significant)\n",
      "NN5_ZNI: Alpha = -0.0018, Beta = 0.1323, Alpha p-value = 0.6389 (Not Significant)\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def calculate_alpha_beta(portfolio_returns, market_returns):\n",
    "    \"\"\"Calculate alpha and beta from a regression.\"\"\"\n",
    "    X = sm.add_constant(market_returns)  # Add a constant for the intercept\n",
    "    model = sm.OLS(portfolio_returns, X).fit()  # Fit the OLS model\n",
    "    alpha = model.params[0]  # Intercept\n",
    "    beta = model.params[1]   # Slope\n",
    "    p_value = model.pvalues[0]  # P-value for alpha\n",
    "    return alpha, beta, p_value\n",
    "\n",
    "# Calculate alpha and beta for each portfolio and the S&P500\n",
    "results = {}\n",
    "for portfolio in [col for col in merged_portfolios.columns if col.endswith('_ZNI')]:\n",
    "    portfolio_returns = merged_portfolios[portfolio]\n",
    "    market_returns = merged_portfolios['SP500_RET']\n",
    "    alpha, beta, p_value = calculate_alpha_beta(portfolio_returns, market_returns)\n",
    "    results[portfolio] = {'alpha': alpha, 'beta': beta, 'p_value': p_value}\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nAlpha and Beta:\")\n",
    "for portfolio, metrics in results.items():\n",
    "    alpha_significance = 'Significant' if metrics['p_value'] < 0.05 else 'Not Significant'\n",
    "    print(f\"{portfolio}: Alpha = {metrics['alpha']:.4f}, Beta = {metrics['beta']:.4f}, Alpha p-value = {metrics['p_value']:.4f} ({alpha_significance})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert alpha and beta results to a DataFrame\n",
    "alpha_beta_df = pd.DataFrame.from_dict(results, orient='index').reset_index()\n",
    "alpha_beta_df.rename(columns={'index': 'Portfolio'}, inplace=True)\n",
    "\n",
    "# Save alpha and beta results to CSV\n",
    "alpha_beta_df.to_csv(r'C:\\Users\\frank\\OneDrive - Escuela Superior de Economia y Negocios\\2. SS 2024\\ML Seminar\\Article redaction\\Tables\\alpha_beta_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmuml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
